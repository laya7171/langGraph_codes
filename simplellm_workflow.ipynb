{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "df76ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bc94212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"gemma3:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e5bf867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8d86c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(QAState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "dd1b8dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_function(state: QAState) -> QAState:\n",
    "    user_query = state[\"question\"]\n",
    "    Prompt = PromptTemplate(\n",
    "        template=\"Answer the question: {user_query}\",\n",
    "        input_variables=[\"user_query\"]\n",
    "    )\n",
    "\n",
    "    formatted_prompt = Prompt.format(user_query=user_query)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "\n",
    "    # Make sure to get response content correctly\n",
    "    state[\"answer\"] = response.content if hasattr(response, \"content\") else response\n",
    "    return state\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "384488fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x194238cb670>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_node(\"qa_function\", qa_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "87b4870e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x194238cb670>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_edge(START , \"qa_function\")\n",
    "graph.add_edge(\"qa_function\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e8b1b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "7974fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = {\n",
    "    'question': \"What is the capital of England?\"\n",
    "}\n",
    "final_state = workflow.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c6de1840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the capital of England?', 'answer': 'The capital of England is **London**. \\n\\nDo you want to know more about London?'}\n"
     ]
    }
   ],
   "source": [
    "print(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275e762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
